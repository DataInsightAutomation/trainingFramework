# python src/train.py     --stage sft     --model_name_or_path /home/dut7042/lun/LLaMA-Factory/Llama-3.2-1B-Instruct     --preprocessing_num_workers 16     --finetuning_type lora     --quantization_method bnb     --template llama3     --flash_attn auto     --dataset_dir data     --eval_dataset custom_test_smaller_alpaca_eval     --cutoff_len 1024     --max_samples 100000     --per_device_eval_batch_size 64    --predict_with_generate True     --max_new_tokens 512     --top_p 0.7     --temperature 0.95     --output_dir saves/Llama-3.2-1B-Instruct/lora/eval_2025-05-20-10-07-46     --trust_remote_code True     --do_predict True  --adapter_name_or_path saves/Llama-3.2-1B-Instruct/lora/sft 
stage: sft
model_name_or_path: /home/dut7042/lun/LLaMA-Factory/Llama-3.2-1B-Instruct
preprocessing_num_workers: 16
finetuning_type: lora
quantization_method: bnb
template: llama3
flash_attn: auto
dataset_dir: data
eval_dataset: custom_test_smaller_alpaca_eval
cutoff_len: 1024
max_samples: 100000
per_device_eval_batch_size: 64
predict_with_generate: True
max_new_tokens: 512
top_p: 0.7
temperature: 0.95
output_dir: saves/Llama-3.2-1B-Instruct/lora/eval_2025-05-20-10-07-46
trust_remote_code: True
do_predict: True
adapter_name_or_path: saves/Llama-3.2-1B-Instruct/lora/sft