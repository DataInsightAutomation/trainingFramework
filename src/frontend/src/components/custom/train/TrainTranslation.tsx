// Translations for the Train component
export const translations = {
  en: {
    trainNewModel: 'Model Configuration',
    modelNameLabel: 'Model Name',
    modelNamePlaceholder: 'Enter model name',
    modelNameError: 'Please provide a model name.',
    modelPathLabel: 'Model Path',
    modelPathPlaceholder: 'Enter model path (e.g., /models/base)',
    modelPathError: 'Please provide a model path.',
    modelPathDescription: 'The server path to the base model or pre-trained model at online you want to use for training.',
    datasetLabel: 'Dataset',
    datasetPlaceholder: 'Enter dataset name or path',
    datasetError: 'Please provide a dataset.',
    trainMethodLabel: 'Training Method',
    selectTrainMethod: 'Select a training method',
    trainMethodError: 'Please select a training method.',
    trainMethodDescription: 'Choose the method to use for training the model. This can include supervised learning, reinforcement learning from human feedback (RLHF), fine-tuning, or knowledge distillation.',
    selectModelName: 'Select a model',
    baseModel: 'Base Model',
    advancedModel: 'Advanced Model',
    supervisedLearning: 'Supervised Learning',
    rlhf: 'Reinforcement Learning from Human Feedback',
    finetuning: 'Fine-tuning',
    distillation: 'Knowledge Distillation',
    startTraining: 'Start Training',
    submitting: 'Submitting...',
    trainingStarted: 'Training job started for model',
    previewCurlCommand: 'Preview Curl Command',
    loadConfig: 'Load Config',
    saveConfig: 'Save Config',
    configSaved: 'Configuration saved successfully',
    configLoaded: 'Configuration loaded successfully',
    curlCommandCopied: 'Curl command copied to clipboard',
    checkStatus: 'Check Status',
    advancedOptions: 'Advanced Options',
    basicOptions: 'Basic Options',
    modelConfiguration: 'Model Configuration',
    finetuningConfiguration: 'Fine-tuning Configuration',
    datasetConfiguration: 'Dataset Configuration',
    trainingConfiguration: 'Training Configuration',
    outputConfiguration: 'Output Configuration',

    // Custom section titles
    modelConfigSection: 'Model Settings',
    finetuningConfigSection: 'Fine-tuning Settings',
    datasetConfigSection: 'Dataset Processing',
    trainingConfigSection: 'Training Parameters',
    outputConfigSection: 'Output Configuration',
    rlhfConfigSection: 'RLHF/PPO Settings',

    // RLHF/PPO Field Labels
    beta_valueLabel: 'Beta Value',
    ftx_gammaLabel: 'FTX Gamma',
    loss_typeLabel: 'Loss Type',
    reward_modelLabel: 'Reward Model',
    score_normLabel: 'Score Norm',
    whiten_rewardsLabel: 'Whiten Rewards',

    // RLHF/PPO Placeholders
    beta_valuePlaceholder: 'Set beta value',
    ftx_gammaPlaceholder: 'Set FTX gamma value',
    loss_typePlaceholder: 'Select loss function',
    reward_modelPlaceholder: 'Select or enter reward model',
    score_normPlaceholder: 'Enable score normalization',
    whiten_rewardsPlaceholder: 'Enable reward whitening',

    // RLHF/PPO Descriptions
    betaValueDescription: 'Value of the beta parameter in the loss.',
    ftxGammaDescription: 'The weight of SFT loss in the final loss.',
    lossTypeDescription: 'The type of the loss function.',
    rewardModelDescription: 'Adapter of the reward model in PPO training.',
    scoreNormDescription: 'Normalizing scores in PPO training.',
    whitenRewardsDescription: 'Whiten the rewards in PPO training.',

    // Field labels
    trust_remote_codeLabel: 'Trust Remote Code',
    stageLabel: 'Training Stage',
    finetuning_typeLabel: 'Finetuning Type',
    lora_rankLabel: 'LoRA Rank',
    lora_targetLabel: 'LoRA Target',
    lora_alphaLabel: 'LoRA Alpha',
    lora_dropoutLabel: 'LoRA Dropout',
    templateLabel: 'Template',
    cutoff_lenLabel: 'Cutoff Length',
    max_samplesLabel: 'Max Samples',
    overwrite_cacheLabel: 'Overwrite Cache',
    preprocessing_num_workersLabel: 'Preprocessing Workers',
    per_device_train_batch_sizeLabel: 'Batch Size',
    gradient_accumulation_stepsLabel: 'Gradient Accumulation Steps',
    learning_rateLabel: 'Learning Rate',
    num_train_epochsLabel: 'Number of Epochs',
    lr_scheduler_typeLabel: 'LR Scheduler Type',
    warmup_ratioLabel: 'Warmup Ratio',
    bf16Label: 'BF16',
    output_dirLabel: 'Output Directory',
    logging_stepsLabel: 'Logging Steps',
    save_stepsLabel: 'Save Steps',
    plot_lossLabel: 'Plot Loss',
    overwrite_output_dirLabel: 'Overwrite Output Directory',

    // Dataset Configuration Fields
    dataset_auto_configLabel: 'Auto Dataset Configuration',
    dataset_ranking_overrideLabel: 'Dataset Ranking Override',
    custom_column_mappingLabel: 'Custom Column Mapping',
    prompt_columnLabel: 'Prompt Column Name',
    query_columnLabel: 'Query Column Name',
    chosen_columnLabel: 'Chosen Column Name',
    rejected_columnLabel: 'Rejected Column Name',
    response_columnLabel: 'Response Column Name',

    // Placeholders
    trust_remote_codePlaceholder: 'Select trust remote code',
    stagePlaceholder: 'Select training stage',
    finetuning_typePlaceholder: 'Select fine-tuning type',
    lora_rankPlaceholder: 'Enter LoRA rank',
    lora_targetPlaceholder: 'Enter LoRA target modules',
    lora_alphaPlaceholder: 'Enter LoRA alpha',
    lora_dropoutPlaceholder: 'Enter LoRA dropout',
    templatePlaceholder: 'Select template',
    cutoff_lenPlaceholder: 'Enter cutoff length',
    max_samplesPlaceholder: 'Enter max samples',
    overwrite_cachePlaceholder: 'Select overwrite cache',
    preprocessing_num_workersPlaceholder: 'Enter preprocessing workers',
    per_device_train_batch_sizePlaceholder: 'Enter batch size',
    gradient_accumulation_stepsPlaceholder: 'Enter gradient accumulation steps',
    learning_ratePlaceholder: 'Enter learning rate',
    num_train_epochsPlaceholder: 'Enter number of epochs',
    lr_scheduler_typePlaceholder: 'Select LR scheduler type',
    warmup_ratioPlaceholder: 'Enter warmup ratio',
    bf16Placeholder: 'Select BF16',
    output_dirPlaceholder: 'Enter output directory',
    logging_stepsPlaceholder: 'Enter logging steps',
    save_stepsPlaceholder: 'Enter save steps',
    plot_lossPlaceholder: 'Select plot loss',
    overwrite_output_dirPlaceholder: 'Select overwrite output directory',

    // Dataset Configuration Placeholders
    dataset_auto_configPlaceholder: 'Enable automatic dataset configuration',
    dataset_ranking_overridePlaceholder: 'Select ranking override',
    custom_column_mappingPlaceholder: 'Enable custom column mapping',
    prompt_columnPlaceholder: 'Enter prompt column name',
    query_columnPlaceholder: 'Enter query column name',
    chosen_columnPlaceholder: 'Enter chosen column name',
    rejected_columnPlaceholder: 'Enter rejected column name',
    response_columnPlaceholder: 'Enter response column name',

    // Dataset Configuration Descriptions
    dataset_auto_configDescription: 'Enable automatic dataset configuration based on training stage',
    dataset_ranking_overrideDescription: 'Override dataset ranking configuration. Auto-detect is recommended.',
    custom_column_mappingDescription: 'Enable custom column mapping (for datasets with non-standard column names)',
    prompt_columnDescription: 'Column name for prompts/instructions',
    query_columnDescription: 'Column name for query/input',
    chosen_columnDescription: 'Column name for chosen responses (RM training only)',
    rejected_columnDescription: 'Column name for rejected responses (RM training only)',
    response_columnDescription: 'Column name for responses (non-RM training)',

    // Add missing basic field translations
    addCustomModel: 'Add custom model: "{input}"',
    enterCustomModelName: 'Enter custom model name or path',
    modelNameDescription: 'Select a model or enter a custom model name/path (e.g., local path or HuggingFace model ID)',
    datasetDescription: "Dataset(s) to use for training",
    
    // Add missing field descriptions
    finetuningMethodDescription: 'Choose how to modify the model parameters during training',
    stageDescription: 'Select the training stage to perform',
    tokenDescription: 'Optional: Provide an API token for authentication with model providers',
    
    // Add missing advanced field descriptions  
    quantizationDescription: 'Use quantization to reduce memory usage during training',
    finetuningTypeDescription: 'Select the method to use for finetuning the model',
    loraRankDescription: 'The rank of the LoRA matrices (higher = more parameters)',
    loraTargetDescription: 'Target modules to apply LoRA to (comma-separated or "all")',
    loraAlphaDescription: 'LoRA scaling parameter (typically 2x the rank)',
    loraDropoutDescription: 'Dropout rate for LoRA layers (0.0 = no dropout)',

    // Training method labels
    supervisedFineTuning: 'Supervised Fine-Tuning (SFT)',
    rlhfTraining: 'Reinforcement Learning from Human Feedback (RLHF)',
    loraFinetuning: 'LoRA Fine-tuning',
    qloraFinetuning: 'QLoRA Fine-tuning',
    fullFinetuning: 'Full Fine-tuning',

    // Add missing basic field labels
    tokenLabel: 'API Token',
    finetuning_methodLabel: 'Finetuning Method',
    reward_modelLabel: 'Reward Model Path',
    
    // Add missing advanced field labels
    quantization_bitLabel: 'Quantization Bits',
    beta_valueLabel: 'Beta Value',
    ftx_gammaLabel: 'FTX Gamma',
    loss_typeLabel: 'Loss Type',

    // Add missing placeholders for advanced fields
    quantization_bitPlaceholder: 'Select quantization level',
    beta_valuePlaceholder: 'Set beta value',
    ftx_gammaPlaceholder: 'Set FTX gamma value',
    loss_typePlaceholder: 'Select loss function',

    // Add descriptions
    rewardModelDescription: 'Path to the trained reward model (required for PPO training). Usually saves/modelname/rm/lora',
    betaValueDescription: 'Beta parameter for RLHF/PPO algorithms (advanced, optional)',
    ftxGammaDescription: 'FTX gamma parameter for RLHF/PPO algorithms (advanced, optional)',
    lossTypeDescription: 'Select the loss function to use for RLHF/PPO training (advanced, optional)',
  },
  zh: {
    // ...existing code...
    rlhfConfigSection: 'RLHF/PPO 设置',

    beta_valueLabel: 'Beta值',
    ftx_gammaLabel: 'FTX Gamma',
    loss_typeLabel: '损失函数',
    reward_modelLabel: '奖励模型',
    score_normLabel: '分数归一化',
    whiten_rewardsLabel: '奖励白化',

    beta_valuePlaceholder: '设置Beta值',
    ftx_gammaPlaceholder: '设置FTX Gamma值',
    loss_typePlaceholder: '选择损失函数',
    reward_modelPlaceholder: '选择或输入奖励模型',
    score_normPlaceholder: '启用分数归一化',
    whiten_rewardsPlaceholder: '启用奖励白化',

    betaValueDescription: '损失中的beta参数值。',
    ftxGammaDescription: '最终损失中SFT损失的权重。',
    lossTypeDescription: '损失函数的类型。',
    rewardModelDescription: 'PPO训练中的奖励模型适配器。',
    scoreNormDescription: 'PPO训练中的分数归一化。',
    whitenRewardsDescription: 'PPO训练中的奖励白化。',
    // ...existing code...
  }
};