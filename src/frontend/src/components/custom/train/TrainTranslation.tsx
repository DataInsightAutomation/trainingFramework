// Translations for the Train component
export const translations = {
  en: {
    trainNewModel: 'Model Configuration',
    modelNameLabel: 'Model Name',
    modelNamePlaceholder: 'Enter model name',
    modelNameError: 'Please provide a model name.',
    modelPathLabel: 'Model Path',
    modelPathPlaceholder: 'Enter model path (e.g., /models/base)',
    modelPathError: 'Please provide a model path.',
    modelPathDescription: 'The server path to the base model or pre-trained model at online you want to use for training.',
    datasetLabel: 'Dataset',
    datasetPlaceholder: 'Enter dataset name or path',
    datasetError: 'Please provide a dataset.',
    trainMethodLabel: 'Training Method',
    selectTrainMethod: 'Select a training method',
    trainMethodError: 'Please select a training method.',
    trainMethodDescription: 'Choose the method to use for training the model. This can include supervised learning, reinforcement learning from human feedback (RLHF), fine-tuning, or knowledge distillation.',
    selectModelName: 'Select a model',
    baseModel: 'Base Model',
    advancedModel: 'Advanced Model',
    supervisedLearning: 'Supervised Learning',
    rlhf: 'Reinforcement Learning from Human Feedback',
    finetuning: 'Fine-tuning',
    distillation: 'Knowledge Distillation',
    startTraining: 'Start Training',
    submitting: 'Submitting...',
    trainingStarted: 'Training job started for model',
    previewCurlCommand: 'Preview Curl Command',
    loadConfig: 'Load Config',
    saveConfig: 'Save Config',
    configSaved: 'Configuration saved successfully',
    configLoaded: 'Configuration loaded successfully',
    curlCommandCopied: 'Curl command copied to clipboard',
    checkStatus: 'Check Status',
    advancedOptions: 'Advanced Options',
    basicOptions: 'Basic Options',
    modelConfiguration: 'Model Configuration',
    finetuningConfiguration: 'Fine-tuning Configuration',
    datasetConfiguration: 'Dataset Configuration',
    trainingConfiguration: 'Training Configuration',
    outputConfiguration: 'Output Configuration',

    // Custom section titles
    modelConfigSection: 'Model Settings',
    finetuningConfigSection: 'Fine-tuning Settings',
    datasetConfigSection: 'Dataset Processing',
    trainingConfigSection: 'Training Parameters',
    outputConfigSection: 'Output Configuration',

    // Field labels
    trust_remote_codeLabel: 'Trust Remote Code',
    stageLabel: 'Training Stage',
    finetuning_typeLabel: 'Finetuning Type',
    lora_rankLabel: 'LoRA Rank',
    lora_targetLabel: 'LoRA Target',
    lora_alphaLabel: 'LoRA Alpha',
    lora_dropoutLabel: 'LoRA Dropout',
    templateLabel: 'Template',
    cutoff_lenLabel: 'Cutoff Length',
    max_samplesLabel: 'Max Samples',
    overwrite_cacheLabel: 'Overwrite Cache',
    preprocessing_num_workersLabel: 'Preprocessing Workers',
    per_device_train_batch_sizeLabel: 'Batch Size',
    gradient_accumulation_stepsLabel: 'Gradient Accumulation Steps',
    learning_rateLabel: 'Learning Rate',
    num_train_epochsLabel: 'Number of Epochs',
    lr_scheduler_typeLabel: 'LR Scheduler Type',
    warmup_ratioLabel: 'Warmup Ratio',
    bf16Label: 'BF16',
    output_dirLabel: 'Output Directory',
    logging_stepsLabel: 'Logging Steps',
    save_stepsLabel: 'Save Steps',
    plot_lossLabel: 'Plot Loss',
    overwrite_output_dirLabel: 'Overwrite Output Directory',

    // Dataset Configuration Fields
    dataset_auto_configLabel: 'Auto Dataset Configuration',
    dataset_ranking_overrideLabel: 'Dataset Ranking Override',
    custom_column_mappingLabel: 'Custom Column Mapping',
    prompt_columnLabel: 'Prompt Column Name',
    query_columnLabel: 'Query Column Name',
    chosen_columnLabel: 'Chosen Column Name',
    rejected_columnLabel: 'Rejected Column Name',
    response_columnLabel: 'Response Column Name',

    // Placeholders
    trust_remote_codePlaceholder: 'Select trust remote code',
    stagePlaceholder: 'Select training stage',
    finetuning_typePlaceholder: 'Select fine-tuning type',
    lora_rankPlaceholder: 'Enter LoRA rank',
    lora_targetPlaceholder: 'Enter LoRA target modules',
    lora_alphaPlaceholder: 'Enter LoRA alpha',
    lora_dropoutPlaceholder: 'Enter LoRA dropout',
    templatePlaceholder: 'Select template',
    cutoff_lenPlaceholder: 'Enter cutoff length',
    max_samplesPlaceholder: 'Enter max samples',
    overwrite_cachePlaceholder: 'Select overwrite cache',
    preprocessing_num_workersPlaceholder: 'Enter preprocessing workers',
    per_device_train_batch_sizePlaceholder: 'Enter batch size',
    gradient_accumulation_stepsPlaceholder: 'Enter gradient accumulation steps',
    learning_ratePlaceholder: 'Enter learning rate',
    num_train_epochsPlaceholder: 'Enter number of epochs',
    lr_scheduler_typePlaceholder: 'Select LR scheduler type',
    warmup_ratioPlaceholder: 'Enter warmup ratio',
    bf16Placeholder: 'Select BF16',
    output_dirPlaceholder: 'Enter output directory',
    logging_stepsPlaceholder: 'Enter logging steps',
    save_stepsPlaceholder: 'Enter save steps',
    plot_lossPlaceholder: 'Select plot loss',
    overwrite_output_dirPlaceholder: 'Select overwrite output directory',

    // Dataset Configuration Placeholders
    dataset_auto_configPlaceholder: 'Enable automatic dataset configuration',
    dataset_ranking_overridePlaceholder: 'Select ranking override',
    custom_column_mappingPlaceholder: 'Enable custom column mapping',
    prompt_columnPlaceholder: 'Enter prompt column name',
    query_columnPlaceholder: 'Enter query column name',
    chosen_columnPlaceholder: 'Enter chosen column name',
    rejected_columnPlaceholder: 'Enter rejected column name',
    response_columnPlaceholder: 'Enter response column name',

    // Dataset Configuration Descriptions
    dataset_auto_configDescription: 'Enable automatic dataset configuration based on training stage',
    dataset_ranking_overrideDescription: 'Override dataset ranking configuration. Auto-detect is recommended.',
    custom_column_mappingDescription: 'Enable custom column mapping (for datasets with non-standard column names)',
    prompt_columnDescription: 'Column name for prompts/instructions',
    query_columnDescription: 'Column name for query/input',
    chosen_columnDescription: 'Column name for chosen responses (RM training only)',
    rejected_columnDescription: 'Column name for rejected responses (RM training only)',
    response_columnDescription: 'Column name for responses (non-RM training)',

    // Add missing basic field translations
    addCustomModel: 'Add custom model: "{input}"',
    enterCustomModelName: 'Enter custom model name or path',
    modelNameDescription: 'Select a model or enter a custom model name/path (e.g., local path or HuggingFace model ID)',
    datasetDescription: "Dataset(s) to use for training",
    
    // Add missing field descriptions
    finetuningMethodDescription: 'Choose how to modify the model parameters during training',
    stageDescription: 'Select the training stage to perform',
    tokenDescription: 'Optional: Provide an API token for authentication with model providers',
    
    // Add missing advanced field descriptions  
    quantizationDescription: 'Use quantization to reduce memory usage during training',
    finetuningTypeDescription: 'Select the method to use for finetuning the model',
    loraRankDescription: 'The rank of the LoRA matrices (higher = more parameters)',
    loraTargetDescription: 'Target modules to apply LoRA to (comma-separated or "all")',
    loraAlphaDescription: 'LoRA scaling parameter (typically 2x the rank)',
    loraDropoutDescription: 'Dropout rate for LoRA layers (0.0 = no dropout)',

    // Training method labels
    supervisedFineTuning: 'Supervised Fine-Tuning (SFT)',
    rlhfTraining: 'Reinforcement Learning from Human Feedback (RLHF)',
    loraFinetuning: 'LoRA Fine-tuning',
    qloraFinetuning: 'QLoRA Fine-tuning',
    fullFinetuning: 'Full Fine-tuning',

    // Add missing basic field labels
    tokenLabel: 'API Token',
    finetuning_methodLabel: 'Finetuning Method',
    reward_modelLabel: 'Reward Model Path',
    
    // Add missing advanced field labels
    quantization_bitLabel: 'Quantization Bits',
    
    // Add missing placeholders for basic fields
    tokenPlaceholder: 'Enter your API token',
    finetuning_methodPlaceholder: 'Select finetuning method',
    reward_modelPlaceholder: 'Enter path to trained reward model',
    
    // Add missing placeholders for advanced fields
    quantization_bitPlaceholder: 'Select quantization level',

    // Add descriptions
    rewardModelDescription: 'Path to the trained reward model (required for PPO training). Usually saves/modelname/rm/lora',
  },
  zh: {
    addCustomModel: '添加自定义模型: "{input}"',
    enterCustomModelName: '输入自定义模型名称或路径',
    modelNameDescription: '选择模型或输入自定义模型名称/路径（例如，本地路径或 HuggingFace 模型 ID）',
    trainNewModel: '模型配置',
    modelNameLabel: '模型名称',
    modelNamePlaceholder: '输入模型名称',
    modelNameError: '请提供模型名称。',
    modelPathLabel: '模型路径',
    modelPathPlaceholder: '输入模型路径（例如，/models/base）',
    modelPathError: '请提供模型路径。',
    modelPathDescription: '您希望用于训练的基础模型或在线预训练模型的服务器路径。',
    datasetLabel: '数据集',
    datasetPlaceholder: '输入数据集名称或路径',
    datasetError: '请提供数据集。',
    trainMethodLabel: '训练方法',
    selectTrainMethod: '选择训练方法',
    trainMethodError: '请选择训练方法。',
    trainMethodDescription: '选择用于训练模型的方法。这可以包括监督学习、人类反馈强化学习（RLHF）、微调或知识蒸馏。',
    selectModelName: '选择模型',
    baseModel: '基础模型',
    advancedModel: '高级模型',
    supervisedLearning: '监督学习',
    rlhf: '人类反馈强化学习',
    finetuning: '微调',
    distillation: '知识蒸馏',
    startTraining: '开始训练',
    submitting: '提交中...',
    trainingStarted: '模型训练任务已开始',
    previewCurlCommand: '预览 Curl 命令',
    loadConfig: '加载配置',
    saveConfig: '保存配置',
    configSaved: '配置保存成功',
    configLoaded: '配置加载成功',
    curlCommandCopied: 'Curl 命令已复制到剪贴板',
    checkStatus: '检查状态',
    advancedOptions: '高级选项',
    basicOptions: '基本选项',
    modelConfiguration: '模型配置',
    finetuningConfiguration: '微调配置',
    datasetConfiguration: '数据集配置',
    trainingConfiguration: '训练配置',
    outputConfiguration: '输出配置',

    // Custom section titles in Chinese
    modelConfigSection: '模型设置',
    finetuningConfigSection: '微调设置',
    datasetConfigSection: '数据集处理',
    trainingConfigSection: '训练参数',
    outputConfigSection: '输出配置',

    // Field labels (Chinese translations would go here)
    trust_remote_codeLabel: '信任远程代码',
    stageLabel: '训练阶段',
    finetuning_typeLabel: '微调类型',
    lora_rankLabel: 'LoRA 排名',
    lora_targetLabel: 'LoRA 目标',
    lora_alphaLabel: 'LoRA Alpha',
    lora_dropoutLabel: 'LoRA Dropout',
    templateLabel: '模板',
    cutoff_lenLabel: '截断长度',
    max_samplesLabel: '最大样本',
    overwrite_cacheLabel: '覆盖缓存',
    preprocessing_num_workersLabel: '预处理工作线程',
    per_device_train_batch_sizeLabel: '每个设备的批量大小',
    gradient_accumulation_stepsLabel: '梯度累积步数',
    learning_rateLabel: '学习率',
    num_train_epochsLabel: '训练轮数',
    lr_scheduler_typeLabel: '学习率调度器类型',
    warmup_ratioLabel: '预热比例',
    bf16Label: 'BF16',
    output_dirLabel: '输出目录',
    logging_stepsLabel: '日志记录步数',
    save_stepsLabel: '保存步数',
    plot_lossLabel: '绘制损失',
    overwrite_output_dirLabel: '覆盖输出目录',

    // Dataset Configuration Fields (Chinese)
    dataset_auto_configLabel: '自动数据集配置',
    dataset_ranking_overrideLabel: '数据集排名覆盖',
    custom_column_mappingLabel: '自定义列映射',
    prompt_columnLabel: '提示列名称',
    query_columnLabel: '查询列名称',
    chosen_columnLabel: '选中列名称',
    rejected_columnLabel: '拒绝列名称',
    response_columnLabel: '响应列名称',

    // Placeholders
    trust_remote_codePlaceholder: '选择信任远程代码',
    stagePlaceholder: '选择训练阶段',
    finetuning_typePlaceholder: '选择微调类型',
    lora_rankPlaceholder: '输入 LoRA 排名',
    lora_targetPlaceholder: '输入 LoRA 目标模块',
    lora_alphaPlaceholder: '输入 LoRA alpha',
    lora_dropoutPlaceholder: '输入 LoRA dropout',
    templatePlaceholder: '选择模板',
    cutoff_lenPlaceholder: '输入截断长度',
    max_samplesPlaceholder: '输入最大样本',
    overwrite_cachePlaceholder: '选择覆盖缓存',
    preprocessing_num_workersPlaceholder: '输入预处理工作线程',
    per_device_train_batch_sizePlaceholder: '输入批量大小',
    gradient_accumulation_stepsPlaceholder: '输入梯度累积步数',
    learning_ratePlaceholder: '输入学习率',
    num_train_epochsPlaceholder: '输入训练轮数',
    lr_scheduler_typePlaceholder: '选择学习率调度器类型',
    warmup_ratioPlaceholder: '输入预热比例',
    bf16Placeholder: '选择 BF16',
    output_dirPlaceholder: '输入输出目录',
    logging_stepsPlaceholder: '输入日志记录步数',
    save_stepsPlaceholder: '输入保存步数',
    plot_lossPlaceholder: '选择绘制损失',
    overwrite_output_dirPlaceholder: '选择覆盖输出目录',

    // Dataset Configuration Placeholders (Chinese)
    dataset_auto_configPlaceholder: '启用自动数据集配置',
    dataset_ranking_overridePlaceholder: '选择排名覆盖',
    custom_column_mappingPlaceholder: '启用自定义列映射',
    prompt_columnPlaceholder: '输入提示列名称',
    query_columnPlaceholder: '输入查询列名称',
    chosen_columnPlaceholder: '输入选中列名称',
    rejected_columnPlaceholder: '输入拒绝列名称',
    response_columnPlaceholder: '输入响应列名称',

    // Dataset Configuration Descriptions (Chinese)
    dataset_auto_configDescription: '基于训练阶段启用自动数据集配置',
    dataset_ranking_overrideDescription: '覆盖数据集排名配置。建议使用自动检测。',
    custom_column_mappingDescription: '启用自定义列映射（适用于非标准列名的数据集）',
    prompt_columnDescription: '提示/指令的列名称',
    query_columnDescription: '查询/输入的列名称',
    chosen_columnDescription: '选中响应的列名称（仅限RM训练）',
    rejected_columnDescription: '拒绝响应的列名称（仅限RM训练）',
    response_columnDescription: '响应的列名称（非RM训练）',

    // Add missing basic field translations
    addCustomModel: '添加自定义模型: "{input}"',
    enterCustomModelName: '输入自定义模型名称或路径',
    modelNameDescription: '选择模型或输入自定义模型名称/路径（例如，本地路径或 HuggingFace 模型 ID）',
    datasetDescription: "用于训练的数据集",
    
    // Add missing field descriptions
    finetuningMethodDescription: '选择训练过程中如何修改模型参数',
    stageDescription: '选择要执行的训练阶段',
    tokenDescription: '可选：提供 API 令牌用于模型提供商的身份验证',
    
    // Add missing advanced field descriptions  
    quantizationDescription: '使用量化减少训练时的内存使用',
    finetuningTypeDescription: '选择用于微调模型的方法',
    loraRankDescription: 'LoRA矩阵的秩（越高=参数越多）',
    loraTargetDescription: '要应用LoRA的目标模块（逗号分隔或"all"）',
    loraAlphaDescription: 'LoRA缩放参数（通常是rank的2倍）',
    loraDropoutDescription: 'LoRA层的丢弃率（0.0 = 无丢弃）',

    // Training method labels
    supervisedFineTuning: '监督式微调 (SFT)',
    rlhfTraining: '人类反馈强化学习 (RLHF)',
    loraFinetuning: 'LoRA微调',
    qloraFinetuning: 'QLoRA微调',
    fullFinetuning: '完全微调',

    // Add missing basic field labels (Chinese)
    tokenLabel: 'API 令牌',
    finetuning_methodLabel: '微调方法',
    reward_modelLabel: '奖励模型路径',
    
    // Add missing advanced field labels (Chinese)
    quantization_bitLabel: '量化位数',
    
    // Add missing placeholders for basic fields (Chinese)
    tokenPlaceholder: '输入您的 API 令牌',
    finetuning_methodPlaceholder: '选择微调方法',
    reward_modelPlaceholder: '输入训练好的奖励模型路径',
    
    // Add missing placeholders for advanced fields (Chinese)
    quantization_bitPlaceholder: '选择量化级别',

    // Add descriptions (Chinese)
    rewardModelDescription: '训练好的奖励模型路径（PPO训练必需）。通常是 saves/modelname/rm/lora',
  }
};